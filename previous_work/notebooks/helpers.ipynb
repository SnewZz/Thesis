{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "from itertools import combinations\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get currently working directory\n",
    "base_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_type='pooling', models_folder='../models'):\n",
    "    ofile = f'{model_type}_model_1m_20interactions.pt'\n",
    "    return torch.load(os.path.join(base_dir, models_folder, ofile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_type='pooling', models_folder='../models'):\n",
    "    ofile = f'{model_type}_model_1m_20interactions.pt'\n",
    "    return torch.save(model, os.path.join(base_dir, models_folder, ofile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StaticVars:\n",
    "    FLOAT_MAX = np.finfo(np.float32).max\n",
    "    INT_MAX = np.iinfo(np.int32).max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StaticVars' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Vincent\\OneDrive\\Bureau\\ULB\\MA2\\MEMO-F524\\Thesis\\previous_work\\notebooks\\helpers.ipynb Cell 6\u001b[0m line \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Vincent/OneDrive/Bureau/ULB/MA2/MEMO-F524/Thesis/previous_work/notebooks/helpers.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mInteractionsInfo\u001b[39;00m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vincent/OneDrive/Bureau/ULB/MA2/MEMO-F524/Thesis/previous_work/notebooks/helpers.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     score \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vincent/OneDrive/Bureau/ULB/MA2/MEMO-F524/Thesis/previous_work/notebooks/helpers.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#     interactions = []\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vincent/OneDrive/Bureau/ULB/MA2/MEMO-F524/Thesis/previous_work/notebooks/helpers.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#     complete_interactions = []\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vincent/OneDrive/Bureau/ULB/MA2/MEMO-F524/Thesis/previous_work/notebooks/helpers.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m#     iter_found = -1\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\Vincent\\OneDrive\\Bureau\\ULB\\MA2\\MEMO-F524\\Thesis\\previous_work\\notebooks\\helpers.ipynb Cell 6\u001b[0m line \u001b[0;36mInteractionsInfo\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vincent/OneDrive/Bureau/ULB/MA2/MEMO-F524/Thesis/previous_work/notebooks/helpers.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#     interactions = []\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vincent/OneDrive/Bureau/ULB/MA2/MEMO-F524/Thesis/previous_work/notebooks/helpers.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#     complete_interactions = []\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vincent/OneDrive/Bureau/ULB/MA2/MEMO-F524/Thesis/previous_work/notebooks/helpers.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m#     iter_found = -1\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vincent/OneDrive/Bureau/ULB/MA2/MEMO-F524/Thesis/previous_work/notebooks/helpers.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     y_loss \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Vincent/OneDrive/Bureau/ULB/MA2/MEMO-F524/Thesis/previous_work/notebooks/helpers.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     proximity_loss \u001b[39m=\u001b[39m StaticVars\u001b[39m.\u001b[39mFLOAT_MAX\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vincent/OneDrive/Bureau/ULB/MA2/MEMO-F524/Thesis/previous_work/notebooks/helpers.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m#     total_loss = StaticVars.FLOAT_MAX\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent/OneDrive/Bureau/ULB/MA2/MEMO-F524/Thesis/previous_work/notebooks/helpers.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, uid, iid, interactions, budget\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m, fobj\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, fconstraint\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'StaticVars' is not defined"
     ]
    }
   ],
   "source": [
    "class InteractionsInfo:\n",
    "    \"\"\"\n",
    "    Represents information about the interactions and counterfactual search for a user-item pair.\n",
    "\n",
    "    Attributes:\n",
    "    - user_id (int): User ID.\n",
    "    - item_id (int): Item ID.\n",
    "    - available_budget (int): Available budget for the search.\n",
    "    - satisfy_objective (bool): Flag indicating whether the objective is satisfied.\n",
    "    - satisfy_constraints (bool): Flag indicating whether constraints are satisfied.\n",
    "    - recommendation (list): List of recommended items.\n",
    "    - interactions (dict): Dictionary containing interaction information (original, initial, best).\n",
    "    - loss (dict): Dictionary containing loss information (initial, best).\n",
    "    - iter_no (dict): Dictionary containing iteration information (initial, best, total).\n",
    "    - budget_spent (dict): Dictionary containing budget spent information (initial, best, total).\n",
    "    - solution_found (bool): Flag indicating whether a solution is found.\n",
    "    - pos (int): Position of the item.\n",
    "    - cfs_dist (int): Counterfactual distance.\n",
    "    - stats_per_cardinality (list): List containing statistics per cardinality.\n",
    "    - max_updated_card (int): Maximum updated cardinality.\n",
    "    - len_interactions (int): Length of original interactions.\n",
    "\n",
    "    Methods:\n",
    "    - __init__: Initializes the InteractionsInfo object.\n",
    "    - __str__: Generates a string representation of the object.\n",
    "    - set_flags: Sets the flags for satisfying objective and constraints.\n",
    "    - needs_update: Checks if an update is needed based on the loss.\n",
    "    - set_values: Sets values for the object based on predictions and losses.\n",
    "    - update_values: Updates values based on predictions, ranking, and losses.\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    y_loss = 1.0\n",
    "    proximity_loss = StaticVars.FLOAT_MAX\n",
    "\n",
    "    def __init__(self, uid, iid, interactions, budget=1000, fobj=True, fconstraint=True, negative_mode=False):\n",
    "        \"\"\"\n",
    "        Initializes an InteractionsInfo object.\n",
    "\n",
    "        Parameters:\n",
    "        - uid (int): User ID.\n",
    "        - iid (int): Item ID.\n",
    "        - interactions (list): List of user-item interactions.\n",
    "        - budget (int): Available budget for the search.\n",
    "        - fobj (bool): Flag indicating whether the objective is satisfied.\n",
    "        - fconstraint (bool): Flag indicating whether constraints are satisfied.\n",
    "        \"\"\"\n",
    "        self.user_id = uid\n",
    "        self.item_id = iid\n",
    "        self.available_budget = budget\n",
    "\n",
    "        self.satisfy_objective = fobj\n",
    "        self.satisfy_contraints = fconstraint\n",
    "\n",
    "        self.recommendation = None\n",
    "        self.interactions = dict(original=interactions, initial=[], best=[])\n",
    "        self.loss = dict(initial=StaticVars.FLOAT_MAX, best=StaticVars.FLOAT_MAX)\n",
    "        self.iter_no = dict(initial=budget, best=budget, total=budget)\n",
    "        self.budget_spent = dict(initial=budget, best=budget, total=budget)\n",
    "\n",
    "        self.solution_found = False\n",
    "        self.pos = StaticVars.INT_MAX\n",
    "        self.cfs_dist = len(interactions)\n",
    "        self.stats_per_cardinality = [0] * len(interactions)\n",
    "        self.max_updated_card = -1\n",
    "\n",
    "        self.len_interactions = len(self.interactions['original'])\n",
    "        self.negative_mode = negative_mode\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Generates a string representation of the InteractionsInfo object.\n",
    "\n",
    "        Returns:\n",
    "        - str: String representation of the object.\n",
    "        \"\"\"\n",
    "        sorted_recommended_items = [\n",
    "            (n[0], n[1].detach().numpy().flatten()[0]) if isinstance(n[1], torch.Tensor)\n",
    "            else (n[0], n[1]) for n in self.recommendation\n",
    "        ]\n",
    "\n",
    "        return (f'\\n'\n",
    "                f'user_id: {self.user_id}, item_id: {self.item_id}\\n'\n",
    "                f'yloss: {round(self.y_loss, 4)}, proximity_loss: {int(self.proximity_loss)}\\n'\n",
    "                f'Item {self.item_id} is in position {self.pos} now!!!\\n'\n",
    "                f'Found in iteration {self.iter_no[\"best\"], {self.budget_spent}} and the interacted items are {self.interactions[\"best\"]}\\n'\n",
    "                f'10-best recommended items {sorted_recommended_items}\\n')\n",
    "\n",
    "    def set_flags(self, do_objective, do_contraints):\n",
    "        \"\"\"\n",
    "        Sets the flags for satisfying objective and constraints.\n",
    "\n",
    "        Parameters:\n",
    "        - do_objective (bool): Flag indicating whether the objective is satisfied.\n",
    "        - do_contraints (bool): Flag indicating whether constraints are satisfied.\n",
    "        \"\"\"\n",
    "        self.satisfy_objective = do_objective\n",
    "        self.satisfy_contraints = do_contraints\n",
    "\n",
    "    def needs_update(self, loss):\n",
    "        \"\"\"\n",
    "        Checks if an update is needed based on the loss.\n",
    "\n",
    "        Parameters:\n",
    "        - loss (dict): Dictionary containing loss information.\n",
    "\n",
    "        Returns:\n",
    "        - bool: True if an update is needed, False otherwise.\n",
    "        \"\"\"\n",
    "        if len(loss):\n",
    "            does_contraints = (not self.satisfy_contraints or self.y_loss > loss['yloss'])\n",
    "            does_objective = (not self.satisfy_objective or self.proximity_loss >= loss['proximity'])\n",
    "\n",
    "            if does_contraints and does_objective: return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def set_values(self, predictions, interacted_items, tot_interacted_items, loss, iter_no, k=10):\n",
    "\n",
    "        # get the ranking position of selected item in the list\n",
    "        rk_data = st.rankdata(-predictions, method='ordinal')\n",
    "        self.pos = rk_data[self.item_id]\n",
    "#         self.recommends = sorted(enumerate(predictions), key=lambda x: x[1], reverse=True)[:k]\n",
    "        accepted_preds = (rk_data <= k).nonzero()\n",
    "        self.recommends = sorted(\n",
    "            zip(predictions[accepted_preds], *accepted_preds), \n",
    "            key=lambda x: x[0], reverse=True)\n",
    "        self.iter_found = iter_no\n",
    "        self.y_loss = loss[0]\n",
    "        self.proximity_loss = loss[1]\n",
    "        self.interactions = interacted_items\n",
    "        self.complete_interactions = tot_interacted_items\n",
    "\n",
    "        self.solution_found = True\n",
    "\n",
    "    def update_values(self, predictions, ranking, interacted_items, loss, iter_no, residual_budget, k):\n",
    "        if (self.negative_mode and ranking[self.item_id] < k) or (not self.negative_mode and ranking[self.item_id] > k):\n",
    "            if loss < self.loss['best']:\n",
    "                self.pos = ranking[self.item_id]\n",
    "                accepted_preds = (ranking <= k).nonzero()\n",
    "                self.recommendation = sorted(\n",
    "                    zip(predictions[accepted_preds], *accepted_preds),\n",
    "                    key=lambda x: x[0], reverse=True)\n",
    "\n",
    "                self.iter_no['best'] = iter_no\n",
    "                self.budget_spent['best'] = self.available_budget - residual_budget\n",
    "                self.loss['best'] = loss\n",
    "                self.interactions['best'] = interacted_items\n",
    "\n",
    "                if not self.solution_found:\n",
    "                    self.iter_no['initial'] = iter_no\n",
    "                    self.budget_spent['initial'] = self.available_budget - residual_budget\n",
    "                    self.loss['initial'] = loss\n",
    "                    self.interactions['initial'] = interacted_items\n",
    "\n",
    "                if self.negative_mode:\n",
    "                    self.cfs_dist = len(self.interactions['best']) - self.len_interactions\n",
    "                    index = len(interacted_items) - self.len_interactions - 1\n",
    "                else:\n",
    "                    self.cfs_dist = self.len_interactions - len(self.interactions['best'])\n",
    "                    index = self.len_interactions - len(interacted_items) - 1\n",
    "\n",
    "                self.stats_per_cardinality[index] = max(\n",
    "                    self.available_budget - residual_budget, self.stats_per_cardinality[index])\n",
    "\n",
    "                self.solution_found = True\n",
    "\n",
    "            self.iter_no['total'] = iter_no\n",
    "        self.budget_spent['total'] = self.available_budget - residual_budget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputeLoss:\n",
    "    def __init__(self, target, original_input, top_k=10, weights=[1, 0, 0], total_CFs=1, negative_mode=False):\n",
    "        self.target_item = target\n",
    "        self.top_k = top_k\n",
    "        self.original_items = original_input\n",
    "        self.total_CFs = total_CFs\n",
    "        self.negative_mode = negative_mode\n",
    "        (self.proximity_weight, self.diversity_weight, self.regularization_weight) = weights\n",
    "\n",
    "    def _compute_yloss(self, target_score, kth_score):\n",
    "        if self.negative_mode:\n",
    "            yloss = max(0, kth_score / target_score - 1.0)\n",
    "        else:\n",
    "            yloss = max(0, target_score / kth_score - 1.0)\n",
    "        return yloss\n",
    "\n",
    "    def _compute_dist(self, x_hat, x1):\n",
    "        \"\"\"Compute weighted distance between two vectors.\"\"\"\n",
    "    #     return sum(abs(x_hat - x1))\n",
    "#         diff = set(x1).difference(set(x_hat))\n",
    "        diff = np.setdiff1d(x1, x_hat)\n",
    "        return len(diff)\n",
    "\n",
    "    def _compute_proximity_loss(self, cfs):\n",
    "        proximity_loss = 0.0\n",
    "        for i in range(self.total_CFs):\n",
    "            proximity_loss += self._compute_dist(cfs, self.original_items)\n",
    "        return proximity_loss / np.multiply(len(self.original_items), self.total_CFs)\n",
    "\n",
    "    def _compute_diversity_loss(self):\n",
    "        proximity_loss = 0.0\n",
    "        return proximity_loss / self.total_CFs\n",
    "\n",
    "    def _compute_regularization_loss(self, x):\n",
    "        \"\"\"Adds a linear equality constraints to the loss functions - to ensure all levels of a categorical variable sums to one\"\"\"\n",
    "        regularization_loss = 0.0\n",
    "        for i in range(self.total_CFs):\n",
    "            pass\n",
    "#             for v in self.encoded_categorical_feature_indexes:\n",
    "#                 regularization_loss += torch.pow((torch.sum(self.cfs[i][v[0]:v[-1]+1]) - 1.0), 2)\n",
    "#             regularization_loss += max(0, x - 1.0)\n",
    "\n",
    "        return regularization_loss\n",
    "\n",
    "    def compute_loss(self, cfs, preds, ranking, total_CFs=1):\n",
    "        \"\"\"Computes the overall loss\"\"\"\n",
    "        yloss = self._compute_yloss(preds[self.target_item], preds[(ranking == self.top_k).nonzero()][0])\n",
    "        proximity_loss = self._compute_proximity_loss(cfs) if self.proximity_weight > 0 else 0.0\n",
    "        diversity_loss = self._compute_diversity_loss() if self.diversity_weight > 0 else 0.0\n",
    "        regularization_loss = self._compute_regularization_loss(yloss) if self.regularization_weight > 0 else 0.0\n",
    "\n",
    "        loss = yloss + (self.proximity_weight * proximity_loss) \\\n",
    "            - (self.diversity_weight * diversity_loss) \\\n",
    "            + (self.regularization_weight * regularization_loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def find_cfs(dataset, model, excluded_item_pos, no_users=None, max_allowed_permutations=None, top_k=10, total_CFs=1):\n",
    "    num_users = no_users or max(dataset.users_ids) + 1\n",
    "    max_perms = max_allowed_permutations or dataset.max_sequence_length\n",
    "\n",
    "    best_tot_loss_data = []\n",
    "    best_yloss_data = []\n",
    "\n",
    "    for user_id in trange(1, num_users):  # dataset.num_users):\n",
    "\n",
    "        seq_size = len(dataset.sequences[dataset.user_ids==user_id])\n",
    "        _total_loss = [None] * seq_size\n",
    "        _yloss = [None] * seq_size\n",
    "\n",
    "        for j in range(seq_size):    \n",
    "            if all(v > 0 for v in dataset.sequences[dataset.user_ids==user_id][j]):    \n",
    "                items_interacted = dataset.sequences[dataset.user_ids==user_id][j]\n",
    "                predictions = -model.predict(items_interacted)\n",
    "                predictions[items_interacted] = StaticVars.FLOAT_MAX\n",
    "\n",
    "                kth_item = predictions.argsort()[top_k - 1]\n",
    "                target_item = predictions.argsort()[min(top_k, int(excluded_item_pos)) - 1]\n",
    "\n",
    "                _total_loss[j] = InteractionsInfo(user_id, target_item)\n",
    "                _yloss[j] = InteractionsInfo(user_id, target_item, fobj=False)\n",
    "\n",
    "                loss = ComputeLoss(target_item, items_interacted, top_k)\n",
    "\n",
    "                counter = 1        \n",
    "\n",
    "                for l in range(len(items_interacted) - 1, max(0, len(items_interacted) - max_perms), -1):\n",
    "                    if _total_loss[j].solution_found: break\n",
    "\n",
    "                    # produce permutations of various interactions\n",
    "                    perm = combinations(items_interacted, l)\n",
    "\n",
    "                    for i in perm:\n",
    "                        # predict next top-k items about to be selected        \n",
    "                        preds = model.predict(i)\n",
    "                        \n",
    "                        # convert logits produced by model, i.e., the probability distribution before normalization, \n",
    "                        # by using softmax\n",
    "                        tensor = torch.from_numpy(preds).float()\n",
    "                        preds = F.softmax(tensor, dim=0)\n",
    "\n",
    "                        yloss = loss._compute_yloss(preds.numpy()[target_item], preds.numpy()[kth_item])\n",
    "                        proximity_loss = loss._compute_proximity_loss(np.asarray(i)[np.newaxis, :])\n",
    "                        \n",
    "                        # keep info about the best solution found depending on an objective function\n",
    "                        if _total_loss[j].needs_update(dict(yloss=yloss, proximity=proximity_loss)):                        \n",
    "                            _total_loss[j].set_values(\n",
    "                                preds, i, items_interacted, [yloss, proximity_loss], counter, top_k)\n",
    "                            \n",
    "#                         if _yloss[j].needs_update(dict(yloss=yloss, proximity=proximity_loss)):\n",
    "#                             _yloss[j].set_values(\n",
    "#                                 preds, i, items_interacted, [yloss, proximity_loss], counter, k)                 \n",
    "\n",
    "                        counter += 1 \n",
    "\n",
    "        best_tot_loss_data.append(_total_loss)\n",
    "        best_yloss_data.append(_yloss)\n",
    "        \n",
    "    return (best_tot_loss_data, best_yloss_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_items_with_jaccard(dataset, target_item, users, jaccard_sims_matrix, number_items=20):\n",
    "    \"\"\"\n",
    "    Find the top best \"number_items\" for each user that, when added to their interactions list,\n",
    "    have the highest Jaccard similarity to the target item based on the given similarity matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - users: List of user indices.\n",
    "    - jaccard_sims_matrix: Jaccard similarity matrix.\n",
    "    - dataset: Dataset containing user sequences and interactions.\n",
    "    - target_item: Index of the target item.\n",
    "    - number_items: The number of items to select for each user.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary where keys are user indices and values are lists of the top best items for each user.\n",
    "    \"\"\"\n",
    "    \n",
    "    best_items_for_users = {}\n",
    "\n",
    "    for user_index in users:\n",
    "        # Get the user's interactions and items not in interactions\n",
    "        user_interactions_list = dataset.sequences[dataset.user_ids == user_index]\n",
    "        \n",
    "        items_not_in_interactions = np.setdiff1d(range(jaccard_sims_matrix.shape[1]), user_interactions_list[0])\n",
    "        \n",
    "        ranked_items = st.rankdata(jaccard_sims_matrix[user_interactions_list[0][target_item], items_not_in_interactions])\n",
    "\n",
    "        # Get the top best items\n",
    "        top_items_indices = np.argsort(ranked_items)[:number_items]\n",
    "\n",
    "        best_items = [list(items_not_in_interactions)[idx] for idx in top_items_indices]\n",
    "\n",
    "        best_items_for_users[user_index] = best_items\n",
    "\n",
    "    return best_items_for_users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In summary, this function is responsible for searching for counterfactual explanations for a specific user's interactions. \n",
    "# It uses a given recommendation model, a search strategy, and other parameters to find items that could replace the target \n",
    "# item at the specified position while considering budget constraints and user preferences. The function stores information \n",
    "# about the search process and the counterfactual solutions in the _total_loss list.\n",
    "def _retrieve_solutions(params):\n",
    "    user_id, d, m, sf, pos, init_budget, top_k, negative_mode, negative_sample, kwargs = params # d is the dataset\n",
    "\n",
    "    _total_loss = []\n",
    "    # print(\"user_id : \", user_id)\n",
    "    seq = d.sequences[d.user_ids == user_id]\n",
    "    for j in range(min(1, len(seq))): \n",
    "        if all(v > 0 for v in seq[j]):\n",
    "            items_interacted = seq[j].copy()\n",
    "            predictions = -m.predict(items_interacted)\n",
    "            # print(predictions)\n",
    "            predictions[items_interacted] = StaticVars.FLOAT_MAX\n",
    "            # print(\"items_interacted : \", items_interacted)\n",
    "            target_item = predictions.argsort()[min(top_k, int(pos)) - 1]\n",
    "\n",
    "            search_info = InteractionsInfo(user_id, target_item, items_interacted, init_budget, negative_mode)\n",
    "            loss = ComputeLoss(target_item, items_interacted, top_k, negative_mode=negative_mode)\n",
    "            if negative_mode:\n",
    "            # give 20 best items instead of interacted items /!\\\n",
    "                strategy = sf(target_item, negative_sample, len(negative_sample), init_budget, m, **kwargs)\n",
    "            else:\n",
    "                strategy = sf(target_item, items_interacted, d.max_sequence_length, init_budget, m, **kwargs)\n",
    "            #Init of the search strategy\n",
    "            counter = 1\n",
    "            budget = strategy.get_init_budget()\n",
    "            while budget > 0:\n",
    "                perm, curr_budget = strategy.next_comb(reverse=search_info.solution_found)\n",
    "                \n",
    "                if perm is None: break  # there is no need to continue searching\n",
    "                \n",
    "                if negative_mode:\n",
    "                    set1 = set(negative_sample) #give 20 best items\n",
    "                    set2 = set(perm)\n",
    "                    perm = items_interacted.tolist() + list(set1 - set2) # bug<\n",
    "\n",
    "                # predict next top-k items about to be selected \n",
    "                preds = m.predict(perm)\n",
    "                preds[perm] = -StaticVars.FLOAT_MAX\n",
    "                rk_data = st.rankdata(-preds, method='ordinal')\n",
    "                computed_loss = loss.compute_loss(perm, preds, rk_data)\n",
    "\n",
    "                # keep info about the best solution found depending on an objective function\n",
    "                search_info.update_values(\n",
    "                    preds, rk_data, perm, computed_loss, counter, curr_budget, top_k)\n",
    "                if hasattr(strategy, 'set_score'):\n",
    "                    reverse_search = strategy.set_score(\n",
    "                        len(items_interacted) - len(perm) - 1,\n",
    "                        preds[target_item],\n",
    "                        preds[(rk_data == top_k).nonzero()][0]\n",
    "                    )\n",
    "\n",
    "                    if reverse_search:\n",
    "                        _total_loss[j].solution_found = False\n",
    "\n",
    "                strategy.reset_costs()\n",
    "                counter += 1\n",
    "        \n",
    "                budget = curr_budget\n",
    "\n",
    "            _total_loss.append(search_info)\n",
    "\n",
    "    return _total_loss\n",
    "\n",
    "\n",
    "\n",
    "# This function is used to find counterfactual explanations (CFs) for a recommender system. \n",
    "def _find_cfs(dataset, model, strategy_func, target_item_pos, negative_mode, sim_matrix, no_users=None, init_budget=1000,\n",
    "              max_allowed_permutations=None, top_k=10, total_CFs=1, num_processes=10, **kwargs):\n",
    "\n",
    "    print(f'The backend used is: {strategy_func.class_name}')\n",
    "\n",
    "    num_users = no_users or max(dataset.users_ids) + 1\n",
    "    best_tot_loss_data = dict.fromkeys(target_item_pos)\n",
    "\n",
    "    with tqdm(total=len(target_item_pos), desc='target position loop') as pbar:\n",
    "        for pos in target_item_pos:\n",
    "            pbar.update(10)\n",
    "            best_item_for_users = None\n",
    "            if negative_mode:\n",
    "                best_item_for_users = find_best_items_with_jaccard(dataset, target_item_pos, list(range(1, num_users+1)), sim_matrix, 20)\n",
    "            best_tot_loss_data[pos] = []\n",
    "            for user_id in trange(1, num_users + 1, desc='users loop', leave=False):\n",
    "                best_item_list = None\n",
    "                if negative_mode:\n",
    "                    best_item_list = best_item_for_users[user_id]\n",
    "                best_tot_loss_data[pos].extend(_retrieve_solutions((\n",
    "                    user_id, dataset, model, strategy_func, pos, init_budget, top_k, negative_mode, best_item_list, kwargs)))\n",
    "\n",
    "    return best_tot_loss_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def convert_res_to_lists(cfs, cnt, non_achieved_target, technique):\n",
    "    for key, values in cfs.items():\n",
    "        total_data = []\n",
    "        cnt[key].setdefault(technique, [])\n",
    "        cfs_no = 0\n",
    "\n",
    "#         for items in values:\n",
    "        for rec in values:\n",
    "            if rec is None: continue\n",
    "\n",
    "#                 if not rec.solution_found or rec.pos < 10:\n",
    "#                     non_achieved_target[key].append(rec.user_id)\n",
    "#                     continue\n",
    "\n",
    "            total_data.append([\n",
    "                len(rec.interactions['original']) - len(rec.interactions['initial']), rec.cfs_dist,\n",
    "                # for boxplot\n",
    "                rec.budget_spent['initial'], rec.budget_spent['best'],\n",
    "                rec.iter_no['initial'], rec.iter_no['best'],\n",
    "                rec.user_id, len(rec.interactions['original'])\n",
    "            ] + rec.stats_per_cardinality)\n",
    "\n",
    "            cfs_no = len(rec.interactions['original'])\n",
    "\n",
    "        cnt[key][technique].append(Counter(item[0] for item in total_data))\n",
    "        cnt[key][technique].append(Counter(item[1] for item in total_data))\n",
    "        cnt[key][technique].append([item[2] for item in total_data])\n",
    "        cnt[key][technique].append([item[3] for item in total_data])\n",
    "        cnt[key][technique].append([item[4] for item in total_data])\n",
    "        cnt[key][technique].append([item[5] for item in total_data])\n",
    "        cnt[key][technique].append([item[6] for item in total_data])\n",
    "        cnt[key][technique].append([item[7] for item in total_data])\n",
    "        cnt[key][technique].append([item[1] for item in total_data])\n",
    "\n",
    "        for i in range(cfs_no):\n",
    "            cnt[key][technique].append([item[8 + i] for item in total_data])\n",
    "\n",
    "    return cnt, non_achieved_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_embeddings_to_cosine_similarity_matrix(E):\n",
    "    \"\"\" \n",
    "    Converts a tensor of n embeddings to an (n, n) tensor of similarities.\n",
    "    \"\"\"\n",
    "    dot = E @ E.t()\n",
    "    norm = torch.norm(E, 2, 1)\n",
    "    x = torch.div(dot, norm)\n",
    "    x = torch.div(x, torch.unsqueeze(norm, -1))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "\n",
    "def embeddings_to_cosine_similarity_matrix(E):\n",
    "    \"\"\" \n",
    "    Converts a a tensor of n embeddings to an (n, n) tensor of similarities.\n",
    "    \"\"\"\n",
    "    similarities = [[cosine_similarity(a, b, dim=0) for a in E] for b in E]\n",
    "#     similarities = list(map(torch.cat, similarities))\n",
    "    similarities = list(map(lambda x: torch.stack(x, dim=-1), similarities))\n",
    "    return torch.stack(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "\n",
    "def compute_sim_matrix(dataset, metric='jaccard', adjusted=False):\n",
    "    # compute the item-item similarity matrix utilizing implicit feedback,\n",
    "    # i.e., whether interacted or not with an item\n",
    "\n",
    "    M = np.zeros((dataset.num_users, dataset.num_items), dtype=bool)\n",
    "    for u in trange(1, dataset.num_users):\n",
    "        np.add.at(\n",
    "            M[u], dataset.item_ids[dataset.user_ids == u],\n",
    "            dataset.ratings[dataset.user_ids == u]\n",
    "        )\n",
    "\n",
    "    if adjusted:\n",
    "        M_u = M.mean(axis=1)\n",
    "        M = M - M_u[:, np.newaxis]\n",
    "\n",
    "    similarity_matrix = 1 - squareform(pdist(M.T, metric))\n",
    "\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def rank_interactions_to_excluded_item_per_user(cfs, sims_matrix):\n",
    "    non_solvable_cases = []\n",
    "    total_data = []\n",
    "\n",
    "    for items in cfs:\n",
    "        for rec in items:\n",
    "            if rec is None: continue\n",
    "\n",
    "            if not rec.solution_found:\n",
    "                non_solvable_cases.append(rec.user_id)\n",
    "                continue\n",
    "\n",
    "            items_rank = st.rankdata(sims_matrix[rec.item_id, rec.complete_interactions])\n",
    "            similarity_rank = len(rec.complete_interactions) - items_rank + 1\n",
    "            del_items_indices = np.where(np.isin(\n",
    "                rec.complete_interactions, \n",
    "                list(set(rec.complete_interactions).difference(set(rec.interactions)))\n",
    "            ))\n",
    "            total_data.extend(sorted(similarity_rank[del_items_indices].astype(int)[-1:]))\n",
    "\n",
    "    return (Counter(total_data), non_solvable_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple class stack that only allows pop and push operations\n",
    "class Stack:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stack = []\n",
    "\n",
    "    def pop(self):\n",
    "        if len(self.stack) < 1:\n",
    "            return None\n",
    "        return self.stack.pop()\n",
    "\n",
    "    def push(self, item):\n",
    "        self.stack.append(item)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.stack)\n",
    "\n",
    "\n",
    "# And a queue that only has enqueue and dequeue operations\n",
    "class Queue:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.queue = []\n",
    "\n",
    "    def enqueue(self, item):\n",
    "        self.queue.append(item)\n",
    "\n",
    "    def dequeue(self):\n",
    "        if len(self.queue) < 1:\n",
    "            return None\n",
    "        return self.queue.pop(0)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.queue)\n",
    "\n",
    "    def clear(self):\n",
    "        del self.queue[:]\n",
    "\n",
    "    def get(self, i):\n",
    "        return self.queue[i]\n",
    "    \n",
    "    def setter(self, i, v):\n",
    "        self.queue[i] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
