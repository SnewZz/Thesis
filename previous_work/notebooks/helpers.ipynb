{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "from tqdm.notebook import trange\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from itertools import combinations\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get currently working directory\n",
    "base_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_type='pooling', models_folder='../models'):\n",
    "    ofile = f'{model_type}_model_1m_20interactions.pt'\n",
    "    return torch.load(os.path.join(base_dir, models_folder, ofile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_type='pooling', models_folder='../models'):\n",
    "    ofile = f'{model_type}_model_1m_20interactions.pt'\n",
    "    return torch.save(model, os.path.join(base_dir, models_folder, ofile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StaticVars:\n",
    "    FLOAT_MAX = np.finfo(np.float32).max\n",
    "    INT_MAX = np.iinfo(np.int32).max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StaticVars' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Vincent\\OneDrive\\Bureau\\ULB\\MA2\\MEMO-F524\\Thesis\\previous_work\\notebooks\\helpers.ipynb Cell 6\u001b[0m line \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Vincent/OneDrive/Bureau/ULB/MA2/MEMO-F524/Thesis/previous_work/notebooks/helpers.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mInteractionsInfo\u001b[39;00m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vincent/OneDrive/Bureau/ULB/MA2/MEMO-F524/Thesis/previous_work/notebooks/helpers.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     score \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vincent/OneDrive/Bureau/ULB/MA2/MEMO-F524/Thesis/previous_work/notebooks/helpers.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#     interactions = []\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vincent/OneDrive/Bureau/ULB/MA2/MEMO-F524/Thesis/previous_work/notebooks/helpers.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#     complete_interactions = []\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vincent/OneDrive/Bureau/ULB/MA2/MEMO-F524/Thesis/previous_work/notebooks/helpers.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m#     iter_found = -1\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\Vincent\\OneDrive\\Bureau\\ULB\\MA2\\MEMO-F524\\Thesis\\previous_work\\notebooks\\helpers.ipynb Cell 6\u001b[0m line \u001b[0;36mInteractionsInfo\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vincent/OneDrive/Bureau/ULB/MA2/MEMO-F524/Thesis/previous_work/notebooks/helpers.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#     interactions = []\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vincent/OneDrive/Bureau/ULB/MA2/MEMO-F524/Thesis/previous_work/notebooks/helpers.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#     complete_interactions = []\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vincent/OneDrive/Bureau/ULB/MA2/MEMO-F524/Thesis/previous_work/notebooks/helpers.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m#     iter_found = -1\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vincent/OneDrive/Bureau/ULB/MA2/MEMO-F524/Thesis/previous_work/notebooks/helpers.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     y_loss \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Vincent/OneDrive/Bureau/ULB/MA2/MEMO-F524/Thesis/previous_work/notebooks/helpers.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     proximity_loss \u001b[39m=\u001b[39m StaticVars\u001b[39m.\u001b[39mFLOAT_MAX\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vincent/OneDrive/Bureau/ULB/MA2/MEMO-F524/Thesis/previous_work/notebooks/helpers.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m#     total_loss = StaticVars.FLOAT_MAX\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vincent/OneDrive/Bureau/ULB/MA2/MEMO-F524/Thesis/previous_work/notebooks/helpers.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, uid, iid, interactions, budget\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m, fobj\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, fconstraint\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'StaticVars' is not defined"
     ]
    }
   ],
   "source": [
    "class InteractionsInfo:\n",
    "    \"\"\"\n",
    "    Represents information about the interactions and counterfactual search for a user-item pair.\n",
    "\n",
    "    Attributes:\n",
    "    - user_id (int): User ID.\n",
    "    - item_id (int): Item ID.\n",
    "    - available_budget (int): Available budget for the search.\n",
    "    - satisfy_objective (bool): Flag indicating whether the objective is satisfied.\n",
    "    - satisfy_constraints (bool): Flag indicating whether constraints are satisfied.\n",
    "    - recommendation (list): List of recommended items.\n",
    "    - interactions (dict): Dictionary containing interaction information (original, initial, best).\n",
    "    - loss (dict): Dictionary containing loss information (initial, best).\n",
    "    - iter_no (dict): Dictionary containing iteration information (initial, best, total).\n",
    "    - budget_spent (dict): Dictionary containing budget spent information (initial, best, total).\n",
    "    - solution_found (bool): Flag indicating whether a solution is found.\n",
    "    - pos (int): Position of the item.\n",
    "    - cfs_dist (int): Counterfactual distance.\n",
    "    - stats_per_cardinality (list): List containing statistics per cardinality.\n",
    "    - max_updated_card (int): Maximum updated cardinality.\n",
    "    - len_interactions (int): Length of original interactions.\n",
    "\n",
    "    Methods:\n",
    "    - __init__: Initializes the InteractionsInfo object.\n",
    "    - __str__: Generates a string representation of the object.\n",
    "    - set_flags: Sets the flags for satisfying objective and constraints.\n",
    "    - needs_update: Checks if an update is needed based on the loss.\n",
    "    - set_values: Sets values for the object based on predictions and losses.\n",
    "    - update_values: Updates values based on predictions, ranking, and losses.\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    y_loss = 1.0\n",
    "    proximity_loss = StaticVars.FLOAT_MAX\n",
    "\n",
    "    def __init__(self, uid, iid, interactions, budget=1000, missing_target_in_topk=False, fobj=True, fconstraint=True):\n",
    "        \"\"\"\n",
    "        Initializes an InteractionsInfo object.\n",
    "\n",
    "        Parameters:\n",
    "        - uid (int): User ID.\n",
    "        - iid (int): Item ID.\n",
    "        - interactions (list): List of user-item interactions.\n",
    "        - budget (int): Available budget for the search.\n",
    "        - missing_target_in_topk: Boolean indicating whether to find CFs for missing (True) or present (False) target items.\n",
    "        - fobj (bool): Flag indicating whether the objective is satisfied.\n",
    "        - fconstraint (bool): Flag indicating whether constraints are satisfied.\n",
    "        \"\"\"\n",
    "        self.user_id = uid\n",
    "        self.item_id = iid\n",
    "        self.available_budget = budget\n",
    "\n",
    "        self.satisfy_objective = fobj\n",
    "        self.satisfy_contraints = fconstraint\n",
    "\n",
    "        self.recommendation = None\n",
    "        if missing_target_in_topk : \n",
    "            self.interactions = dict(original=interactions, initial=interactions, best=interactions)\n",
    "        else : \n",
    "            self.interactions = dict(original=interactions, initial=[], best=[])\n",
    "        self.loss = dict(initial=StaticVars.FLOAT_MAX, best=StaticVars.FLOAT_MAX)\n",
    "        self.iter_no = dict(initial=budget, best=budget, total=budget)\n",
    "        self.budget_spent = dict(initial=budget, best=budget, total=budget)\n",
    "\n",
    "        self.solution_found = False\n",
    "        self.pos = StaticVars.INT_MAX\n",
    "        self.cfs_dist = len(interactions)\n",
    "        self.stats_per_cardinality = [0] * len(interactions)\n",
    "        self.max_updated_card = -1\n",
    "\n",
    "        self.len_interactions = len(self.interactions['original'])\n",
    "        self.missing_target_in_topk = missing_target_in_topk\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Generates a string representation of the InteractionsInfo object.\n",
    "\n",
    "        Returns:\n",
    "        - str: String representation of the object.\n",
    "        \"\"\"\n",
    "        sorted_recommended_items = [\n",
    "            (n[0], n[1].detach().numpy().flatten()[0]) if isinstance(n[1], torch.Tensor)\n",
    "            else (n[0], n[1]) for n in self.recommendation\n",
    "        ]\n",
    "\n",
    "        return (f'\\n'\n",
    "                f'user_id: {self.user_id}, item_id: {self.item_id}\\n'\n",
    "                f'yloss: {round(self.y_loss, 4)}, proximity_loss: {int(self.proximity_loss)}\\n'\n",
    "                f'Item {self.item_id} is in position {self.pos} now!!!\\n'\n",
    "                f'Found in iteration {self.iter_no[\"best\"], {self.budget_spent}} and the interacted items are {self.interactions[\"best\"]}\\n'\n",
    "                f'10-best recommended items {sorted_recommended_items}\\n')\n",
    "\n",
    "    def set_flags(self, do_objective, do_contraints):\n",
    "        \"\"\"\n",
    "        Sets the flags for satisfying objective and constraints.\n",
    "\n",
    "        Parameters:\n",
    "        - do_objective (bool): Flag indicating whether the objective is satisfied.\n",
    "        - do_contraints (bool): Flag indicating whether constraints are satisfied.\n",
    "        \"\"\"\n",
    "        self.satisfy_objective = do_objective\n",
    "        self.satisfy_contraints = do_contraints\n",
    "\n",
    "    def needs_update(self, loss):\n",
    "        \"\"\"\n",
    "        Checks if an update is needed based on the loss.\n",
    "\n",
    "        Parameters:\n",
    "        - loss (dict): Dictionary containing loss information.\n",
    "\n",
    "        Returns:\n",
    "        - bool: True if an update is needed, False otherwise.\n",
    "        \"\"\"\n",
    "        if len(loss):\n",
    "            does_contraints = (not self.satisfy_contraints or self.y_loss > loss['yloss'])\n",
    "            does_objective = (not self.satisfy_objective or self.proximity_loss >= loss['proximity'])\n",
    "\n",
    "            if does_contraints and does_objective: return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    \n",
    "    def set_values(self, predictions, interacted_items, tot_interacted_items, loss, iter_no, k=10):\n",
    "        \"\"\"\n",
    "        Set values for recommendation results and evaluation metrics.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "\n",
    "        Sets the following attributes:\n",
    "            - self.pos (int): Ranking position of the selected item in the list of predictions.\n",
    "            - self.recommends (list): List of top-k recommendations sorted by prediction scores.\n",
    "            - self.iter_found (int): Iteration number where solution was found.\n",
    "            - self.y_loss (float): Loss value related to the predicted outcome.\n",
    "            - self.proximity_loss (float): Proximity loss value.\n",
    "            - self.interactions (list): List of items interacted with by the user.\n",
    "            - self.complete_interactions (int): Total number of interacted items by all users.\n",
    "            - self.solution_found (bool): Flag indicating if a solution was found.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # get the ranking position of selected item in the list\n",
    "        rk_data = st.rankdata(-predictions, method='ordinal')\n",
    "        self.pos = rk_data[self.item_id]\n",
    "#         self.recommends = sorted(enumerate(predictions), key=lambda x: x[1], reverse=True)[:k]\n",
    "        accepted_preds = (rk_data <= k).nonzero()\n",
    "        self.recommends = sorted(\n",
    "            zip(predictions[accepted_preds], *accepted_preds), \n",
    "            key=lambda x: x[0], reverse=True)\n",
    "        self.iter_found = iter_no\n",
    "        self.y_loss = loss[0]\n",
    "        self.proximity_loss = loss[1]\n",
    "        self.interactions = interacted_items\n",
    "        self.complete_interactions = tot_interacted_items\n",
    "\n",
    "        self.solution_found = True\n",
    "\n",
    "    def update_values(self, predictions, ranking, interacted_items, loss, iter_no, residual_budget, k):\n",
    "        if (self.missing_target_in_topk and ranking[self.item_id] <= k) or (not self.missing_target_in_topk and ranking[self.item_id] > k):\n",
    "            # print(\"update_values!!!\")\n",
    "            if loss < self.loss['best']:\n",
    "                # print(\"better loss!!!\",  loss)\n",
    "                self.pos = ranking[self.item_id]\n",
    "                accepted_preds = (ranking <= k).nonzero()\n",
    "                self.recommendation = sorted(\n",
    "                    zip(predictions[accepted_preds], *accepted_preds),\n",
    "                    key=lambda x: x[0], reverse=True)\n",
    "\n",
    "                self.iter_no['best'] = iter_no\n",
    "                self.budget_spent['best'] = self.available_budget - residual_budget\n",
    "                self.loss['best'] = loss\n",
    "                self.interactions['best'] = interacted_items\n",
    "\n",
    "                if not self.solution_found:\n",
    "                    self.iter_no['initial'] = iter_no\n",
    "                    self.budget_spent['initial'] = self.available_budget - residual_budget\n",
    "                    self.loss['initial'] = loss\n",
    "                    self.interactions['initial'] = interacted_items\n",
    "\n",
    "                if self.missing_target_in_topk:\n",
    "                    self.cfs_dist = len(self.interactions['best']) - self.len_interactions\n",
    "                    index = len(interacted_items) - self.len_interactions - 1\n",
    "\n",
    "                    if index < 0 or index >= len(self.stats_per_cardinality):\n",
    "                        print(\"Bug !!!!!\")\n",
    "                        print(\"index : \", index)\n",
    "                        print(interacted_items)\n",
    "                        print(self.interactions)\n",
    "                        print(self.len_interactions)\n",
    "                else:\n",
    "                    self.cfs_dist = self.len_interactions - len(self.interactions['best'])\n",
    "                    index = self.len_interactions - len(interacted_items) - 1\n",
    "\n",
    "                self.stats_per_cardinality[index] = max(\n",
    "                    self.available_budget - residual_budget, self.stats_per_cardinality[index])\n",
    "\n",
    "                self.solution_found = True\n",
    "\n",
    "            self.iter_no['total'] = iter_no\n",
    "        self.budget_spent['total'] = self.available_budget - residual_budget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputeLoss:\n",
    "    def __init__(self, target, original_input, top_k=10, weights=[1, 0, 0], total_CFs=1, missing_target_in_topk=False):\n",
    "        self.target_item = target\n",
    "        self.top_k = top_k\n",
    "        self.original_items = original_input\n",
    "        self.total_CFs = total_CFs\n",
    "        self.missing_target_in_topk = missing_target_in_topk\n",
    "        (self.proximity_weight, self.diversity_weight, self.regularization_weight) = weights\n",
    "\n",
    "    def _compute_yloss(self, target_score, kth_score):\n",
    "        if self.missing_target_in_topk:\n",
    "            yloss = max(0, kth_score / target_score - 1.0)\n",
    "        else:\n",
    "            yloss = max(0, target_score / kth_score - 1.0)\n",
    "        return yloss\n",
    "\n",
    "    def _compute_dist(self, x_hat, x1):\n",
    "        \"\"\"Compute weighted distance between two vectors.\"\"\"\n",
    "        diff = np.setdiff1d(x1, x_hat)\n",
    "        return len(diff)\n",
    "\n",
    "    def _compute_proximity_loss(self, cfs):\n",
    "        proximity_loss = 0.0\n",
    "        for i in range(self.total_CFs):\n",
    "            if self.missing_target_in_topk:\n",
    "                proximity_loss += self._compute_dist(self.original_items, cfs)\n",
    "            else:\n",
    "                proximity_loss += self._compute_dist(cfs, self.original_items)\n",
    "\n",
    "        return proximity_loss / np.multiply(len(self.original_items), self.total_CFs)\n",
    "\n",
    "    def _compute_diversity_loss(self):\n",
    "        proximity_loss = 0.0\n",
    "        return proximity_loss / self.total_CFs\n",
    "\n",
    "    def _compute_regularization_loss(self, x):\n",
    "        \"\"\"Adds a linear equality constraints to the loss functions - to ensure all levels of a categorical variable sums to one\"\"\"\n",
    "        regularization_loss = 0.0\n",
    "        for i in range(self.total_CFs):\n",
    "            pass\n",
    "#             for v in self.encoded_categorical_feature_indexes:\n",
    "#                 regularization_loss += torch.pow((torch.sum(self.cfs[i][v[0]:v[-1]+1]) - 1.0), 2)\n",
    "#             regularization_loss += max(0, x - 1.0)\n",
    "\n",
    "        return regularization_loss\n",
    "\n",
    "    def compute_loss(self, cfs, preds, ranking, total_CFs=1):\n",
    "        \"\"\"Computes the overall loss\"\"\"\n",
    "        yloss = self._compute_yloss(preds[self.target_item], preds[(ranking == self.top_k).nonzero()][0])\n",
    "        proximity_loss = self._compute_proximity_loss(cfs) if self.proximity_weight > 0 else 0.0\n",
    "        diversity_loss = self._compute_diversity_loss() if self.diversity_weight > 0 else 0.0\n",
    "        regularization_loss = self._compute_regularization_loss(yloss) if self.regularization_weight > 0 else 0.0\n",
    "\n",
    "        loss = yloss + (self.proximity_weight * proximity_loss) \\\n",
    "            - (self.diversity_weight * diversity_loss) \\\n",
    "            + (self.regularization_weight * regularization_loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "\n",
    "def find_sample_with_jaccard(target_item, user_interactions, jaccard_sims_matrix, k=20, worst_items = False):\n",
    "    \"\"\"\n",
    "    Find the top k items for a user that have the highest Jaccard similarity to the target item\n",
    "    based on the given similarity matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - target_item: Index of the target item.\n",
    "    - user_interactions: List of items interacted with by the user.\n",
    "    - jaccard_sims_matrix: Jaccard similarity matrix.\n",
    "    - k: The number of items to select.\n",
    "\n",
    "    Returns:\n",
    "    - A list of the top k items for the user.\n",
    "    \"\"\"\n",
    "    # Adjust the indices in user_interactions to start from 0 (in the movielens dataset, index starts at 1)\n",
    "    user_interactions_adjusted = np.array(user_interactions) - 1\n",
    "\n",
    "    target_item_adjusted = target_item - 1\n",
    "    \n",
    "    items_not_in_interactions = np.setdiff1d(range(jaccard_sims_matrix.shape[1]), user_interactions_adjusted)\n",
    "\n",
    "    # Get the Jaccard similarities between the target item and items not in interactions\n",
    "    ranked_items = st.rankdata(jaccard_sims_matrix[target_item_adjusted, items_not_in_interactions])\n",
    "\n",
    "    # Get the top k items\n",
    "    if worst_items :\n",
    "        top_items_indices = np.argsort(ranked_items)[-k:]\n",
    "    else :\n",
    "        top_items_indices = np.argsort(ranked_items)[:k]\n",
    "\n",
    "    # Get the actual item indices (starting from 1)\n",
    "    best_items = list(items_not_in_interactions[top_items_indices])\n",
    "\n",
    "    best_items = np.array(best_items)\n",
    "\n",
    "    best_items += 1\n",
    "\n",
    "    best_items = best_items.tolist()\n",
    "\n",
    "\n",
    "    return best_items\n",
    "\n",
    "def find_sample_with_recommender(target_item, user_interactions, model, k=20):\n",
    "    predictions = -model.predict(target_item)\n",
    "    predictions[user_interactions] = StaticVars.FLOAT_MAX\n",
    "    sorted_predictions = predictions.argsort()\n",
    "    best_items = sorted_predictions[:k//2]\n",
    "\n",
    "    remaining_items = sorted_predictions[k//2:-len(user_interactions)]\n",
    "    random_indices = np.random.choice(remaining_items, k//2, replace=False)\n",
    "\n",
    "    combined_items = np.concatenate((best_items, random_indices))\n",
    "\n",
    "    np.random.shuffle(combined_items)\n",
    "\n",
    "    combined_items = combined_items.tolist()\n",
    "    \n",
    "    return combined_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _retrieve_solutions(params):\n",
    "    \"\"\"\n",
    "    Retrieve counterfactual solutions for a specific target position for a specific user. Using a given recommendation model, a search strategy it will search for \n",
    "    CFs while considering budget constraints and user preferences (sim_matrix) to explain : \n",
    "     - if missing_target_in_topk is True, why the target position in the top recommendation of the model is not in the top_k recommendations of the user. (pos will then be greater than top_k)\n",
    "     - if missing_target_in_topk is False, why the target position in the top recommendation of the model is in the top_k recommendations of the user. (pos will then be lower or equals than top_k)\n",
    "\n",
    "    Parameters:\n",
    "    - params (tuple): Tuple containing the following elements:\n",
    "        - user_id: User ID for whom counterfactual solutions are sought (lowest id starts at 1).\n",
    "        - d: Dataset object containing user-item interactions.\n",
    "        - m: Recommender system model.\n",
    "        - sf: Counterfactual search strategy function.\n",
    "        - pos: Target position for which CFs are generated.\n",
    "        - init_budget: Initial budget for CF generation.\n",
    "        - top_k: Number of top-ranked items to consider.\n",
    "        - missing_target_in_topk: Boolean indicating whether to find CFs for missing (True) or present (False) target items.\n",
    "        - sim_matrix: Item-item similarity matrix used in the CF generation process.\n",
    "        - kwargs: Additional keyword arguments to be passed to the CF generation process.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of InteractionsInfo object containing information about the search process and counterfactual solutions for the specified user.\n",
    "    \"\"\"\n",
    "    user_id, d, m, sf, pos, init_budget, top_k, missing_target_in_topk, sim_matrix, kwargs = params\n",
    "    \n",
    "    _total_loss = []\n",
    "    seq = d.sequences[d.user_ids == user_id]\n",
    "    for j in range(min(1, len(seq))): \n",
    "        if all(v > 0 for v in seq[j]): # To remove subsequences shorter than \"max_sequence_length\" left-padded with zeros (see dataset documentation).\n",
    "            items_interacted = seq[j].copy()\n",
    "\n",
    "            predictions = -m.predict(items_interacted) # We put a \"-\" to after sort the list in increasing order (the higher the value of the prediction, the higher its ranking in the recommendations)\n",
    "            predictions[items_interacted] = StaticVars.FLOAT_MAX # This is done to obtain the items with which the user interacted at the end of the recommendations of the RS.\n",
    "            predictions[0] = StaticVars.FLOAT_MAX # This is done because there is no item at index 0 (item_id starts at 1)\n",
    "            # predictions = predictions[1:]\n",
    "            negative_sample = [] # Will be used in the missing_target_in_topk case to select a sample of best candidate items to add to have the target item in top k\n",
    "\n",
    "            if missing_target_in_topk:\n",
    "                # give the index in the predictions array that corresponds to the target pos in the sorted recommendation\n",
    "\n",
    "                target_item = predictions.argsort()[max(top_k + 1, int(pos))]\n",
    "                \n",
    "                # We chose this length to match with the size of the sample in the other mode (when missing_target_in_topk is False).\n",
    "                size_sample = len(items_interacted)\n",
    "\n",
    "                # give a sample of the dataset of items similar to the target item.\n",
    "                negative_sample = find_sample_with_jaccard(target_item, items_interacted, sim_matrix, size_sample)\n",
    "\n",
    "            else:\n",
    "                # give the index in the predictions array that corresponds to the target pos in the sorted recommendations\n",
    "                target_item = predictions.argsort()[min(top_k, int(pos))]\n",
    "                \n",
    "            search_info = InteractionsInfo(user_id, target_item, items_interacted, init_budget, missing_target_in_topk=missing_target_in_topk)\n",
    "            loss = ComputeLoss(target_item, items_interacted, top_k, missing_target_in_topk=missing_target_in_topk)\n",
    "\n",
    "            if missing_target_in_topk:\n",
    "                # In this mode as we want to search to add item to have the target_item in the top recommendations, we use the strategies to choose items among a sample of items not in the interacted items.\n",
    "                strategy = sf(target_item, negative_sample, len(negative_sample), init_budget, m, **kwargs)\n",
    "                \n",
    "            else:\n",
    "                strategy = sf(target_item, items_interacted, d.max_sequence_length, init_budget, m, **kwargs)\n",
    "            \n",
    "            # start of the search strategy\n",
    "            counter = 1\n",
    "            budget = strategy.get_init_budget()\n",
    "            while budget > 0:\n",
    "                perm, curr_budget = strategy.next_comb(reverse=search_info.solution_found)\n",
    "                if perm is None: break \n",
    "                \n",
    "                if missing_target_in_topk:\n",
    "                    # As the strategies were initially designed to select items to remove from the interactions to reject the target item out of top_k, and we gave a sample \n",
    "                    # of similar items of the target in the \"missing_target_in_topk\" mode, we now want to have the items that strategy decided to remove to add them to the \n",
    "                    # interacted items\n",
    "                    set1 = set(negative_sample)\n",
    "                    set2 = set(perm)\n",
    "                    items_to_add = list(set1 - set2)\n",
    "\n",
    "                    perm = items_interacted.tolist() + items_to_add # maybe a bug TODO test it\n",
    "\n",
    "                # update the predictions of each items (and then the rank) based on the \"new\" items interactions of the user\n",
    "                preds = m.predict(perm)\n",
    "                preds[perm] = -StaticVars.FLOAT_MAX\n",
    "                preds[0] = -StaticVars.FLOAT_MAX\n",
    "                print(\"is equal to 0? : \", preds[0])\n",
    "                # /!\\ the rankdata function give 1 as the lower number for a rank (and not 0)\n",
    "                rk_data = st.rankdata(-preds, method='ordinal')\n",
    "                \n",
    "                computed_loss = loss.compute_loss(perm, preds, rk_data)\n",
    "\n",
    "                # keep info about the best solution found depending on an objective function\n",
    "                search_info.update_values(\n",
    "                    preds, rk_data, perm, computed_loss, counter, curr_budget, top_k)\n",
    "                \n",
    "                if hasattr(strategy, 'set_score'):\n",
    "                    reverse_search = strategy.set_score(\n",
    "                        len(items_interacted) - len(perm) - 1,\n",
    "                        preds[target_item],\n",
    "                        preds[(rk_data == top_k).nonzero()][0]\n",
    "                    )\n",
    "\n",
    "                    if reverse_search:\n",
    "                        _total_loss[j].solution_found = False\n",
    "\n",
    "                strategy.reset_costs()\n",
    "                counter += 1\n",
    "        \n",
    "                budget = curr_budget\n",
    "\n",
    "\n",
    "            _total_loss.append(search_info)\n",
    "\n",
    "    return _total_loss\n",
    "import heapq\n",
    "def _find_cfs(dataset, model, strategy_func, target_item_pos, missing_target_in_topk, sim_matrix, no_users=None, init_budget=1000,\n",
    "              max_allowed_permutations=None, top_k=10, total_CFs=1, num_processes=10, **kwargs):\n",
    "    \"\"\"\n",
    "    Find counterfactual explanations (CFs) for a recommender system.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset: Dataset object containing user-item interactions.\n",
    "    - model: Recommender system model.\n",
    "    - strategy_func: Counterfactual explanation strategy function.\n",
    "    - target_item_pos (list): List of target positions for which CFs are to be generated.\n",
    "    - missing_target_in_topk (bool): Indicates whether we search CFs for \"why\" the target item is missing from the top-k recommendations (True) or present in the top-k recommendations (False).\n",
    "    - sim_matrix: Item-item similarity matrix used in the CF generation process.\n",
    "    - no_users (int, optional): Number of users to consider, default is None (use maximum user ID in the dataset). /!\\ in the dataset the number in the ids starts at 1, it is not 0-based like usually in python\n",
    "    - init_budget (int): Initial budget for CF generation, default is 1000.\n",
    "    - max_allowed_permutations (int, optional): Maximum allowed permutations during CF generation, default is None.\n",
    "    - top_k (int): Number of top-ranked items to consider during CF generation, default is 10.\n",
    "    - total_CFs (int): Total number of CFs to generate for each target position, default is 1.\n",
    "    - num_processes (int): Number of processes to use for parallelization, default is 10.\n",
    "    - **kwargs: Additional keyword arguments to be passed to the CF generation process.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing CFs for each target position and user ID. Each key represents a target position and the value is a list extends by the result of the \"_retrieve_solutions\" function for each user ID.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f'The backend used is: {strategy_func.class_name}')\n",
    "\n",
    "    num_users = no_users or max(dataset.user_ids) # In case of no_users = None we take the max in the user_ids of the dataset (not \"max(dataset.user_ids) + 1\" because the ids start at 1) \n",
    "    best_tot_loss_data = dict.fromkeys(target_item_pos)\n",
    "\n",
    "    with tqdm(total=len(target_item_pos), desc='target position loop') as pbar:\n",
    "        for pos in target_item_pos:\n",
    "            pbar.update(10)\n",
    "            best_tot_loss_data[pos] = []\n",
    "            for user_id in trange(1, num_users + 1, desc='users loop', leave=False):\n",
    "                retrieved_solution = _retrieve_solutions((\n",
    "                    user_id, dataset, model, strategy_func, pos, init_budget, top_k, missing_target_in_topk, sim_matrix, kwargs))\n",
    "                best_tot_loss_data[pos].extend(retrieved_solution)\n",
    "\n",
    "    return best_tot_loss_data\n",
    "\n",
    "def _find_specific_cfs_(dataset, model, strategy_func, specific_pos, missing_target_in_topk, sim_matrix, specific_uid, init_budget=1000, top_k=10, **kwargs):\n",
    "    return _retrieve_solutions((specific_uid, dataset, model, strategy_func, specific_pos, init_budget, top_k, missing_target_in_topk, sim_matrix, kwargs))\n",
    "\n",
    "def retrieve_solutions_specific_sequence(user_id, d, m, sf, init_budget, top_k, missing_target_in_topk, sim_matrix, items_interacted, target_item, negative_sample, **kwargs):\n",
    "    _total_loss = []\n",
    "    \n",
    "    predictions = -m.predict(items_interacted) # We put a \"-\" to after sort the list in increasing order (the higher the value of the prediction, the higher its ranking in the recommendations)\n",
    "    predictions[items_interacted] = StaticVars.FLOAT_MAX # This is done to obtain the items with which the user interacted at the end of the recommendations of the RS.\n",
    "    print(\"check first pred = 0\", predictions[0])\n",
    "    top_21 = heapq.nlargest(21, predictions)\n",
    "    print(\"biggest value : \", top_21)\n",
    "    top_21 = heapq.nsmallest(21, predictions)\n",
    "    print(\"smallest value : \", top_21)\n",
    "    search_info = InteractionsInfo(user_id, target_item, items_interacted, init_budget, missing_target_in_topk=missing_target_in_topk)\n",
    "    loss = ComputeLoss(target_item, items_interacted, top_k, missing_target_in_topk = missing_target_in_topk)\n",
    "    \n",
    "    if missing_target_in_topk:\n",
    "        # In this mode as we want to search to add item to have the target_item in the top recommendations, we use the strategies to choose items among a sample of items not in the interacted items.\n",
    "        strategy = sf(target_item, negative_sample, len(negative_sample), init_budget, m, **kwargs)\n",
    "    else:\n",
    "        strategy = sf(target_item, items_interacted, d.max_sequence_length, init_budget, m, **kwargs)\n",
    "\n",
    "    # start of the search strategy \n",
    "    counter = 1\n",
    "    budget = strategy.get_init_budget()\n",
    "    \n",
    "    while budget > 0:\n",
    "        perm, curr_budget = strategy.next_comb(reverse=search_info.solution_found)\n",
    "        if perm is None: break \n",
    "        \n",
    "        if missing_target_in_topk:\n",
    "            # As the strategies were initially designed to select items to remove from the interactions to reject the target item out of top_k, and we gave a sample \n",
    "            # of similar items of the target in the \"missing_target_in_topk\" mode, we now want to have the items that strategy decided to remove to add them to the \n",
    "            # interacted items\n",
    "            set1 = set(negative_sample)\n",
    "            set2 = set(perm)\n",
    "            items_to_add = list(set1 - set2)\n",
    "            if isinstance(items_interacted, list):\n",
    "                perm = items_interacted + items_to_add\n",
    "            else: \n",
    "                perm = items_interacted.tolist() + items_to_add # maybe a bug TODO test it\n",
    "        \n",
    "        # update the predictions of each items (and then the rank) based on the \"new\" items interactions of the user\n",
    "        preds = m.predict(perm)\n",
    "        preds[perm] = -StaticVars.FLOAT_MAX\n",
    "        # /!\\ the rankdata function give 1 as the lower number for a rank (and not 0)\n",
    "        rk_data = st.rankdata(-preds, method='ordinal')\n",
    "        \n",
    "        computed_loss = loss.compute_loss(perm, preds, rk_data)\n",
    "        # keep info about the best solution found depending on an objective function\n",
    "        \n",
    "        # print(\"target_item : \", target_item)\n",
    "        # print(\"rank : \", rk_data[target_item])\n",
    "        # print(\"lost : \", computed_loss)\n",
    "\n",
    "        search_info.update_values(\n",
    "            preds, rk_data, perm, computed_loss, counter, curr_budget, top_k)\n",
    "        \n",
    "        if hasattr(strategy, 'set_score'):\n",
    "            reverse_search = strategy.set_score(\n",
    "                len(items_interacted) - len(perm) - 1,\n",
    "                preds[target_item],\n",
    "                preds[(rk_data == top_k).nonzero()][0]\n",
    "            )\n",
    "            if reverse_search:\n",
    "                _total_loss[j].solution_found = False\n",
    "        \n",
    "        strategy.reset_costs()\n",
    "        counter += 1\n",
    "        budget = curr_budget\n",
    "    \n",
    "    _total_loss.append(search_info)\n",
    "\n",
    "    return _total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def convert_res_to_lists(cfs, cnt, non_achieved_target, technique, missing_target_in_topk):\n",
    "    \"\"\"\n",
    "    Convert counterfactual results to lists for analysis.\n",
    "\n",
    "    Parameters:\n",
    "    - cfs (dict): Counterfactual results for a specific strategy.\n",
    "    - cnt (dict): Dictionary for counting statistics.\n",
    "    - non_achieved_target (list): [Not used?] List for tracking cases where the target is not achieved.\n",
    "    - technique (str): Name of the counterfactual strategy.\n",
    "    - missing_target_in_topk: Boolean indicating whether to find CFs for missing (True) or present (False) target items in top k.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Updated count dictionary and list of non-achieved target cases.\n",
    "    \"\"\"\n",
    "    \n",
    "    # the key is the position and values are lists of info on each user\n",
    "    for key, values in cfs.items():\n",
    "        total_data = []\n",
    "        # Ensures that if the key does not exist in cnt, it is created with an empty list as default value\n",
    "        cnt[key].setdefault(technique, [])\n",
    "        cfs_no = 0\n",
    "\n",
    "        for rec in values:\n",
    "            if rec is None: continue\n",
    "            if missing_target_in_topk:\n",
    "                total_data.append([\n",
    "                    len(rec.interactions['initial']) - len(rec.interactions['original']), \n",
    "                    rec.cfs_dist,\n",
    "                    # for boxplot\n",
    "                    rec.budget_spent['initial'], rec.budget_spent['best'],\n",
    "                    rec.iter_no['initial'], rec.iter_no['best'],\n",
    "                    rec.user_id, len(rec.interactions['original'])\n",
    "                ] + rec.stats_per_cardinality)\n",
    "            else:\n",
    "                total_data.append([\n",
    "                    len(rec.interactions['original']) - len(rec.interactions['initial']), \n",
    "                    rec.cfs_dist,\n",
    "                    # for boxplot\n",
    "                    rec.budget_spent['initial'], rec.budget_spent['best'],\n",
    "                    rec.iter_no['initial'], rec.iter_no['best'],\n",
    "                    rec.user_id, len(rec.interactions['original'])\n",
    "                ] + rec.stats_per_cardinality)\n",
    "\n",
    "            cfs_no = len(rec.interactions['original'])\n",
    "        cnt[key][technique].append([item[0] for item in total_data])\n",
    "        cnt[key][technique].append([item[1] for item in total_data])\n",
    "        cnt[key][technique].append([item[2] for item in total_data])\n",
    "        cnt[key][technique].append([item[3] for item in total_data])\n",
    "        cnt[key][technique].append([item[4] for item in total_data])\n",
    "        cnt[key][technique].append([item[5] for item in total_data])\n",
    "        cnt[key][technique].append([item[6] for item in total_data])\n",
    "        cnt[key][technique].append([item[7] for item in total_data])\n",
    "        cnt[key][technique].append([item[1] for item in total_data])\n",
    "\n",
    "        for i in range(cfs_no):\n",
    "            cnt[key][technique].append([item[8 + i] for item in total_data])\n",
    "\n",
    "    return cnt, non_achieved_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_embeddings_to_cosine_similarity_matrix(E):\n",
    "    \"\"\"\n",
    "    Converts a tensor of n embeddings to an (n, n) tensor of cosine similarities.\n",
    "\n",
    "    Parameters:\n",
    "    - E (torch.Tensor): Tensor of n embeddings.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: Cosine similarity matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    dot = E @ E.t()\n",
    "    norm = torch.norm(E, 2, 1)\n",
    "    x = torch.div(dot, norm)\n",
    "    x = torch.div(x, torch.unsqueeze(norm, -1))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "\n",
    "def embeddings_to_cosine_similarity_matrix(E):\n",
    "    \"\"\" \n",
    "    Converts a a tensor of n embeddings to an (n, n) tensor of similarities.\n",
    "    \"\"\"\n",
    "    similarities = [[cosine_similarity(a, b, dim=0) for a in E] for b in E]\n",
    "    similarities = list(map(lambda x: torch.stack(x, dim=-1), similarities))\n",
    "    return torch.stack(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "\n",
    "def compute_sim_matrix(dataset, metric='jaccard', adjusted=False):\n",
    "    \"\"\"\n",
    "    Computes the item-item similarity matrix utilizing implicit feedback i.e., whether interacted or not with an item\n",
    "\n",
    "    Parameters:\n",
    "    - dataset: Dataset object containing user-item interactions.\n",
    "    - metric (str): Similarity metric to use, default is 'jaccard'.\n",
    "    - adjusted (bool): Whether to use adjusted similarity scores, default is False.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Item-item similarity matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a matrix M representing user-item interactions\n",
    "    M = np.zeros((dataset.num_users - 1, dataset.num_items - 1), dtype=bool)\n",
    "    for u in trange(1, dataset.num_users):\n",
    "        np.add.at(\n",
    "            M[u-1], (dataset.item_ids[dataset.user_ids == u]) - 1,\n",
    "            dataset.ratings[dataset.user_ids == u]\n",
    "        )\n",
    "\n",
    "    # Adjust the matrix if specified (it centers data around zero)\n",
    "    if adjusted:\n",
    "        M_u = M.mean(axis=1)\n",
    "        M = M - M_u[:, np.newaxis]\n",
    "\n",
    "    # Compute the similarity matrix based on the chosen metric, by subtracting these values \n",
    "    # from 1, we obtain a similarity measure where higher values indicate greater similarity.\n",
    "    # We take the transposed because pdist compute distance between cols and we want between rows.\n",
    "    similarity_matrix = 1 - squareform(pdist(M.T, metric))\n",
    "\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def rank_interactions_to_excluded_item_per_user(cfs, sims_matrix):\n",
    "    non_solvable_cases = []\n",
    "    total_data = []\n",
    "\n",
    "    for items in cfs:\n",
    "        for rec in items:\n",
    "            if rec is None: continue\n",
    "\n",
    "            if not rec.solution_found:\n",
    "                non_solvable_cases.append(rec.user_id)\n",
    "                continue\n",
    "\n",
    "            items_rank = st.rankdata(sims_matrix[rec.item_id, rec.complete_interactions])\n",
    "            similarity_rank = len(rec.complete_interactions) - items_rank + 1\n",
    "            del_items_indices = np.where(np.isin(\n",
    "                rec.complete_interactions, \n",
    "                list(set(rec.complete_interactions).difference(set(rec.interactions)))\n",
    "            ))\n",
    "            total_data.extend(sorted(similarity_rank[del_items_indices].astype(int)[-1:]))\n",
    "\n",
    "    return (Counter(total_data), non_solvable_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evaluation_dataframe(dataset, model, strategy, budget, target_items_pos, top_k, sim_matrix, csv_file_name = None, alpha = None, normalization = None):\n",
    "    \"\"\"\n",
    "        Creates a DataFrame that contains the user_id, the target posisition, the original interactions of \n",
    "        this user and the counterfactual explanation found by the strategy given as parameter.\n",
    "\n",
    "        Args:\n",
    "            dataset : The dataset containing users' interaction sequences.\n",
    "            model : The recommender system.\n",
    "            strategy : The strategy used to find the cfs.\n",
    "            budget (int) : The budget allocated for the strategy.\n",
    "            target_items_pos (list) : The list of position of the items in the list of recommendation for which we need cfs.\n",
    "            csv_file_name (string) : If a name is specified, it creates a csv file of the df with this name.\n",
    "        \n",
    "        Returns:\n",
    "            Dataframe : A df with the result.\n",
    "        \"\"\"\n",
    "    evaluation_df = pd.DataFrame(columns=['user_id', 'target_pos', 'target_item', 'original_interactions', 'best_interactions', 'cfs', 'len_cfs'])\n",
    "    # list_test = [1001, 1169, 1340]\n",
    "    for user_id in tqdm(range(1, max(dataset.user_ids))):\n",
    "        for target_pos in target_items_pos:\n",
    "            if alpha is None or normalization is None :\n",
    "                specific_cfs = _find_specific_cfs_(dataset, model, strategy, target_pos, False, sim_matrix, user_id, budget, top_k)\n",
    "            else :\n",
    "                specific_cfs = _find_specific_cfs_(dataset, model, strategy, target_pos, False, sim_matrix, user_id, budget, top_k, alpha = alpha, normalization = normalization)\n",
    "            \n",
    "            user_sequences = dataset.sequences[dataset.user_ids == user_id]\n",
    "            \n",
    "            for j in range (min(1, len(user_sequences))):\n",
    "                if all(v > 0 for v in user_sequences[j]):\n",
    "                    original_interactions = user_sequences[j].copy()\n",
    "                    best_interactions = specific_cfs[j].interactions['best']\n",
    "                    item_id = specific_cfs[j].item_id\n",
    "                    items_removed = np.setdiff1d(original_interactions, best_interactions)\n",
    "                    # new_lign = {'user_id': user_id, 'target_pos': target_pos, 'target_item': item_id, 'original_interactions': original_interactions.tolist(), 'best_interactions': best_interactions.tolist(),'cfs': items_removed.tolist(), 'len_cfs': len(items_removed)}\n",
    "                    new_lign = {'user_id': user_id, 'target_pos': target_pos, 'target_item': item_id, 'original_interactions': original_interactions, 'best_interactions': best_interactions,'cfs': items_removed.tolist(), 'len_cfs': len(items_removed)}\n",
    "                    evaluation_df = evaluation_df.append(new_lign, ignore_index=True)\n",
    "    if csv_file_name is not None : \n",
    "        path = os.getcwd()\n",
    "        csv_path = os.path.join(path, \"csv\")\n",
    "        if not os.path.exists(csv_path):\n",
    "            os.makedirs(csv_path)\n",
    "        \n",
    "        evaluation_df.to_csv(os.path.join(csv_path, csv_file_name), index=False)\n",
    "                    \n",
    "    return evaluation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def replace_items_if_missing(items_removed, target_list):\n",
    "    items_to_replace = set(items_removed) - set(target_list)\n",
    "    for i, item in enumerate(items_to_replace):\n",
    "        j = i\n",
    "        while target_list[j] in items_removed:\n",
    "            j += 1\n",
    "        target_list[j] = item\n",
    "    # should maybe shuffle?\n",
    "\n",
    "def are_the_same(list1, list2):\n",
    "    sorted_list1 = sorted(list1)\n",
    "    sorted_list2 = sorted(list2)\n",
    "\n",
    "    return sorted_list1 == sorted_list2\n",
    "    \n",
    "def evaluate_reverse_mode(csv_file_name, output_name, model, dataset, strategy, budget, len_sample, sim_matrix, top_k, alpha = None, normalization = None):\n",
    "    \n",
    "    good_cases_df = pd.DataFrame(columns=['user_id', 'target_pos', 'mid_pos', 'last_pos', 'original_interactions', 'items_removed', 'exclusion_interactions', 'inclusion_interactions', 'worst_jac'])\n",
    "    good_but_not_same_cases_df = pd.DataFrame(columns=['user_id', 'target_pos', 'mid_pos', 'last_pos', 'original_interactions', 'items_removed', 'exclusion_interactions', 'inclusion_interactions', 'worst_jac'])\n",
    "    wrong_cases_df = pd.DataFrame(columns=['user_id', 'target_pos', 'mid_pos', 'last_pos', 'original_interactions', 'items_removed', 'exclusion_interactions', 'inclusion_interactions', 'worst_jac'])\n",
    "    \n",
    "    path = os.getcwd()\n",
    "    csv_path = os.path.join(path, \"csv\")\n",
    "    file = os.path.join(csv_path, csv_file_name)\n",
    "    \n",
    "    if not os.path.exists(file) :\n",
    "        print(\"Need the csv file : \" + csv_file_name + \". Please run the create_evaluation_dataframe function with this specific strategy to generate the csv file.\")\n",
    "    \n",
    "    else :\n",
    "        df = pd.read_csv(file)\n",
    "\n",
    "        counter = 0\n",
    "        with tqdm(total=df.shape[0], desc='csv reading...') as pbar:\n",
    "            for index, row in df.iterrows():\n",
    "                user_id = row[\"user_id\"]\n",
    "                target_pos = row[\"target_pos\"]\n",
    "                target_item = row[\"target_item\"]\n",
    "                original_interactions = ast.literal_eval(row[\"original_interactions\"])\n",
    "                exclusion_interactions = ast.literal_eval(row[\"best_interactions\"])\n",
    "                items_removed = ast.literal_eval(row[\"cfs\"])\n",
    "\n",
    "                # print(\"user_id :\", user_id)\n",
    "                # print(\"target_pos :\", target_pos)\n",
    "\n",
    "                if len(exclusion_interactions) == 0 : \n",
    "                    print(\"empty\", user_id)\n",
    "                    continue\n",
    "                \n",
    "                # preds = m.predict(perm)\n",
    "                # preds[perm] = -StaticVars.FLOAT_MAX\n",
    "                # #/!\\ the rankdata function give 1 as the lower number for a rank (and not 0)\n",
    "                # rk_data = st.rankdata(-preds, method='ordinal')\n",
    "                \n",
    "            \n",
    "\n",
    "                predictions_reverse = -model.predict(exclusion_interactions)\n",
    "                predictions_reverse[exclusion_interactions] = StaticVars.FLOAT_MAX\n",
    "                target_pos_reverse = np.where(predictions_reverse.argsort() == target_item)[0][0] \n",
    "\n",
    "                worst_jaccard_sample = find_sample_with_jaccard(target_item, exclusion_interactions, sim_matrix, len_sample, worst_items = True)\n",
    "\n",
    "                if len(items_removed) >= len_sample:\n",
    "                    print(f\"Sequence skipped, too much items removed for user_id {user_id}.\")\n",
    "                    continue\n",
    "                \n",
    "                if not set(items_removed) <= set(worst_jaccard_sample):\n",
    "                    replace_items_if_missing(items_removed, worst_jaccard_sample)\n",
    "                \n",
    "                if alpha is None or normalization is None : \n",
    "                    worst_jacc_search_info = retrieve_solutions_specific_sequence(user_id, dataset, model, strategy, budget, top_k, True, sim_matrix, exclusion_interactions, target_item, worst_jaccard_sample)\n",
    "                else:\n",
    "                    worst_jacc_search_info = retrieve_solutions_specific_sequence(user_id, dataset, model, strategy, budget, top_k, True, sim_matrix, exclusion_interactions, target_item, worst_jaccard_sample, alpha=0.5, normalization='default')\n",
    "\n",
    "                inclusion_interactions = worst_jacc_search_info[0].interactions['best']\n",
    "                inclusion_predictions = -model.predict(inclusion_interactions)\n",
    "                inclusion_predictions[inclusion_interactions] = StaticVars.FLOAT_MAX\n",
    "                last_target_pos = np.where(inclusion_predictions.argsort() == target_item)[0][0]\n",
    "\n",
    "                if last_target_pos <= target_pos :\n",
    "                    if inclusion_interactions == original_interactions :\n",
    "                        counter = counter + 1\n",
    "                        new_lign = {'user_id': user_id, 'target_pos': target_pos, 'mid_pos' : target_pos_reverse, 'last_pos' : last_target_pos, 'original_interactions' : original_interactions, 'items_removed' : items_removed, 'exclusion_interactions' : exclusion_interactions, 'inclusion_interactions' : inclusion_interactions,'worst_jac' : worst_jaccard_sample, 'same_interactions' : True}\n",
    "                        good_cases_df = good_cases_df.append(new_lign, ignore_index=True) \n",
    "                    elif len(inclusion_interactions) <= len(original_interactions):\n",
    "                        counter = counter + 1\n",
    "                        new_lign = {'user_id': user_id, 'target_pos': target_pos, 'mid_pos' : target_pos_reverse, 'last_pos' : last_target_pos, 'original_interactions' : original_interactions, 'items_removed' : items_removed, 'exclusion_interactions' : exclusion_interactions, 'inclusion_interactions' : inclusion_interactions,'worst_jac' : worst_jaccard_sample, 'same_interactions' : False}\n",
    "                        good_but_not_same_cases_df = good_but_not_same_cases_df.append(new_lign, ignore_index=True) \n",
    "                    else : \n",
    "                        new_lign = {'user_id': user_id, 'target_pos': target_pos, 'mid_pos' : target_pos_reverse, 'last_pos' : last_target_pos, 'original_interactions' : original_interactions, 'items_removed' : items_removed, 'exclusion_interactions' : exclusion_interactions, 'inclusion_interactions' : inclusion_interactions, 'worst_jac' : worst_jaccard_sample, 'same_interactions' : False}\n",
    "                        wrong_cases_df = wrong_cases_df.append(new_lign, ignore_index=True) \n",
    "                else : \n",
    "                    new_lign = {'user_id': user_id, 'target_pos': target_pos, 'mid_pos' : target_pos_reverse, 'last_pos' : last_target_pos, 'original_interactions' : original_interactions, 'items_removed' : items_removed, 'exclusion_interactions' : exclusion_interactions, 'inclusion_interactions' : inclusion_interactions, 'worst_jac' : worst_jaccard_sample, 'same_interactions' : False}\n",
    "                    wrong_cases_df = wrong_cases_df.append(new_lign, ignore_index=True) \n",
    "                \n",
    "                pbar.update(1)\n",
    "\n",
    "        good_cases_df.to_csv(\"./csv/evaluation_reverse_mode/\" + output_name + \"_good_cases.csv\", index=False)  \n",
    "        good_but_not_same_cases_df.to_csv(\"./csv/evaluation_reverse_mode/\" + output_name + \"_good_but_not_same_cases.csv\", index=False)  \n",
    "        wrong_cases_df.to_csv(\"./csv/evaluation_reverse_mode/\" + output_name + \"_wrong_cases.csv\", index=False)  \n",
    "            \n",
    "\n",
    "    return counter, df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def evaluate_sampling_method(csv_file_name, model, dataset, strategy, len_sample, sim_matrix, top_k, sampling_method = \"jaccard\", alpha = None, normalization = None):\n",
    "    result_df = pd.DataFrame(columns=['user_id', 'target_pos','inclusion_pos', 'original_interactions', 'exclusion_interactions', 'inclusion_interactions', 'length_exclusion_cfs', 'length_inclusion_cfs', 'inclusion_removed_items'])\n",
    "    \n",
    "    path = os.getcwd()\n",
    "    csv_path = os.path.join(path, \"csv\")\n",
    "    file = os.path.join(csv_path, csv_file_name)\n",
    "    \n",
    "    if not os.path.exists(file) :\n",
    "        print(\"Need the csv file : \" + csv_file_name + \". Please run the create_evaluation_dataframe function with this specific strategy to generate the csv file.\")\n",
    "    \n",
    "    else :\n",
    "        df = pd.read_csv(file)\n",
    "\n",
    "        counter = 0\n",
    "        with tqdm(total=df.shape[0], desc='csv reading...') as pbar:\n",
    "            for index, row in df.iterrows():\n",
    "                user_id = row[\"user_id\"]\n",
    "                target_pos = row[\"target_pos\"]\n",
    "                target_item = row[\"target_item\"]\n",
    "                original_interactions = ast.literal_eval(row[\"original_interactions\"])\n",
    "                exclusion_interactions = ast.literal_eval(row[\"best_interactions\"])\n",
    "                items_removed = ast.literal_eval(row[\"cfs\"])\n",
    "                length_cfs = row[\"len_cfs\"]\n",
    "\n",
    "                if sampling_method == \"jaccard\":\n",
    "                    inclusion_sample = find_sample_with_jaccard(target_item, exclusion_interactions, sim_matrix, len_sample)\n",
    "                elif sampling_method == \"rs\":\n",
    "                    inclusion_sample = find_sample_with_recommender(target_item, exclusion_interactions, model, k=20)\n",
    "\n",
    "\n",
    "                if alpha is None or normalization is None : \n",
    "                    inclusion_search_info = retrieve_solutions_specific_sequence(user_id, dataset, model, strategy, 1048576, top_k, True, sim_matrix, exclusion_interactions, target_item, inclusion_sample)\n",
    "                else:\n",
    "                    inclusion_search_info = retrieve_solutions_specific_sequence(user_id, dataset, model, strategy, 1048576, top_k, True, sim_matrix, exclusion_interactions, target_item, inclusion_sample, alpha=0.5, normalization='default')\n",
    "\n",
    "\n",
    "                inclusion_interactions = inclusion_search_info[0].interactions['best']\n",
    "                inclusion_predictions = -model.predict(inclusion_interactions)\n",
    "                inclusion_predictions[inclusion_interactions] = StaticVars.FLOAT_MAX\n",
    "                last_target_pos = np.where(inclusion_predictions.argsort() == target_item)[0][0]\n",
    "                inclusion_removed_items = np.setdiff1d(inclusion_interactions, exclusion_interactions)\n",
    "                length_inclusion_cfs = len(np.setdiff1d(inclusion_interactions, exclusion_interactions))\n",
    "\n",
    "                new_lign = {'user_id': user_id, 'target_pos': target_pos, 'inclusion_pos' : last_target_pos, 'original_interactions' : original_interactions, 'exclusion_interactions' : exclusion_interactions, 'inclusion_interactions' : inclusion_interactions,'length_exclusion_cfs' : length_cfs, 'length_inclusion_cfs' : length_inclusion_cfs, 'inclusion_removed_items' : inclusion_removed_items }\n",
    "                result_df = result_df.append(new_lign, ignore_index=True) \n",
    "\n",
    "        result_df.to_csv(\"./csv./evaluate_sampling_\" + sampling_method + \".csv\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ast\n",
    "# def test_evaluate_reverse_mode(csv_file_name, model, dataset, strategy, len_sample, sim_matrix, top_k, alpha = None, normalization = None):\n",
    "#     path = os.getcwd()\n",
    "#     csv_path = os.path.join(path, \"csv\")\n",
    "#     file = os.path.join(csv_path, csv_file_name)\n",
    "    \n",
    "#     if not os.path.exists(file) :\n",
    "#         print(\"Need the csv file : \" + csv_file_name + \". Please run the create_evaluation_dataframe function with this specific strategy to generate the csv file.\")\n",
    "    \n",
    "#     else :\n",
    "#         df = pd.read_csv(file)\n",
    "#         row = df.iloc[0]\n",
    "#         user_id = row[\"user_id\"]\n",
    "#         target_pos = row[\"target_pos\"]\n",
    "    \n",
    "#         user_sequences = dataset.sequences[dataset.user_ids == user_id]\n",
    "#         user_sequences = [sequence for sequence in user_sequences if all(value > 0 for value in sequence)]\n",
    "#         original_interactions = user_sequences[0]\n",
    "#         original_interactions = ast.literal_eval(row[\"original_interactions\"])\n",
    "#         print(\"original :\", original_interactions)\n",
    "    \n",
    "#         brute_force_specific_cfs = _find_specific_cfs_(dataset, model, strategy, target_pos, False, sim_matrix, user_id, 1048576, top_k)\n",
    "#         best_interactions = brute_force_specific_cfs[0].interactions['best']\n",
    "#         best_interactions = ast.literal_eval(row[\"best_interactions\"])\n",
    "#         print(\"best:\", best_interactions)\n",
    "    \n",
    "#         items_removed = np.setdiff1d(original_interactions, best_interactions)\n",
    "#         items_removed = ast.literal_eval(row[\"cfs\"])\n",
    "#         print(\"items_removed\", items_removed)\n",
    "        \n",
    "#         predictions = -model.predict(original_interactions)\n",
    "#         predictions[original_interactions] = StaticVars.FLOAT_MAX\n",
    "#         target_item = predictions.argsort()[min(top_k, target_pos)] # TODO retrieve it from the csv\n",
    "#         print(\"target\", target_item)\n",
    "    \n",
    "#         if len(best_interactions) == 0 : \n",
    "#             print(\"empty\", user_id)\n",
    "            \n",
    "#         predictions_reverse = -model.predict(best_interactions)\n",
    "#         predictions_reverse[best_interactions] = StaticVars.FLOAT_MAX\n",
    "#         pos_target_item_reverse = np.where(predictions_reverse.argsort() == target_item)[0][0] #bug maybe?\n",
    "#         print(\"new pos\",  pos_target_item_reverse)\n",
    "    \n",
    "#         worst_jaccard_sample = find_sample_with_jaccard(target_item, best_interactions, sim_matrix, len_sample, worst_items = True)\n",
    "#         print(\"worst_jac\", worst_jaccard_sample )\n",
    "        \n",
    "#         if len(items_removed) >= len_sample:\n",
    "#             print(f\"Sequence skipped, too much items removed for user_id {user_id}.\")\n",
    "        \n",
    "#         if not set(items_removed) <= set(worst_jaccard_sample):\n",
    "#             replace_items_if_missing(items_removed, worst_jaccard_sample)\n",
    "#             print(\"new worst\", worst_jaccard_sample)\n",
    "    \n",
    "#             if alpha is None or normalization is None : \n",
    "#                 worst_jacc_search_info = retrieve_solutions_specific_sequence(user_id, dataset, model, strategy, 1048576, top_k, True, sim_matrix, best_interactions, target_item, worst_jaccard_sample)\n",
    "#             else:\n",
    "#                 worst_jacc_search_info = retrieve_solutions_specific_sequence(user_id, dataset, model, strategy, 1048576, top_k, True, sim_matrix, best_interactions, target_item, worst_jaccard_sample, alpha=0.5, normalization='default')\n",
    "        \n",
    "#         print(\"neg_original_interactions\", worst_jacc_search_info[0].interactions['best'])\n",
    "    \n",
    "#         last_predictions_reverse = -model.predict(worst_jacc_search_info[0].interactions['best'])\n",
    "#         last_predictions_reverse[worst_jacc_search_info[0].interactions['best']] = StaticVars.FLOAT_MAX\n",
    "#         last_pos_target_item_reverse = np.where(predictions_reverse.argsort() == target_item)[0][0] #bug maybe?\n",
    "#         print(\"last pos\",  last_pos_target_item_reverse)\n",
    "    \n",
    "#         # worst_jacc_cfs = np.setdiff1d( worst_jacc_search_info[0].interactions['best'], best_interactions)\n",
    "        \n",
    "#         if are_the_same(worst_jacc_search_info[0].interactions['best'], original_interactions):\n",
    "#             # counter = counter + 1\n",
    "#             print(\"Hopefully they are the same!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_evaluate_reverse_mode(user_id, csv_file_name, model, dataset, strategy, len_sample, sim_matrix, top_k, alpha = None, normalization = None):\n",
    "    \n",
    "    target_pos = 9\n",
    "\n",
    "    user_sequences = dataset.sequences[dataset.user_ids == user_id]\n",
    "    user_sequences = [sequence for sequence in user_sequences if all(value > 0 for value in sequence)]\n",
    "    original_interactions = user_sequences[0]\n",
    "    print(\"original :\", original_interactions)\n",
    "\n",
    "    brute_force_specific_cfs = _find_specific_cfs_(dataset, model, strategy, target_pos, False, sim_matrix, user_id, 1048576, top_k)\n",
    "    best_interactions = brute_force_specific_cfs[0].interactions['best']\n",
    "    print(\"best:\", best_interactions)\n",
    "\n",
    "    items_removed = np.setdiff1d(original_interactions, best_interactions)\n",
    "    print(\"items_removed\", items_removed)\n",
    "    \n",
    "    predictions = -model.predict(original_interactions)\n",
    "    predictions[original_interactions] = StaticVars.FLOAT_MAX\n",
    "    target_item = predictions.argsort()[min(top_k, target_pos)] # TODO retrieve it from the csv\n",
    "    print(\"target\", target_item)\n",
    "\n",
    "    if len(best_interactions) == 0 : \n",
    "        print(\"empty\", user_id)\n",
    "        \n",
    "    predictions_reverse = -model.predict(best_interactions)\n",
    "    predictions_reverse[best_interactions] = StaticVars.FLOAT_MAX\n",
    "    pos_target_item_reverse = np.where(predictions_reverse.argsort() == target_item)[0][0] #bug maybe?\n",
    "    print(\"new pos\",  pos_target_item_reverse)\n",
    "\n",
    "    worst_jaccard_sample = find_sample_with_jaccard(target_item, best_interactions, sim_matrix, len_sample, worst_items = True)\n",
    "    print(\"worst_jac\", worst_jaccard_sample )\n",
    "    \n",
    "    if len(items_removed) >= len_sample:\n",
    "        print(f\"Sequence skipped, too much items removed for user_id {user_id}.\")\n",
    "    \n",
    "    if not set(items_removed) <= set(worst_jaccard_sample):\n",
    "        replace_items_if_missing(items_removed, worst_jaccard_sample)\n",
    "        print(\"new worst\", worst_jaccard_sample)\n",
    "        #bug ??\n",
    "        if alpha is None or normalization is None : \n",
    "            worst_jacc_search_info = _find_specific_cfs_(dataset, model, strategy, pos_target_item_reverse, True, sim_matrix, user_id, 1048576, top_k)\n",
    "            worst_jacc_search_info = retrieve_solutions_specific_sequence(user_id, dataset, model, strategy, 1048576, top_k, True, sim_matrix, best_interactions, target_item, worst_jaccard_sample)\n",
    "        else:\n",
    "            worst_jacc_search_info = retrieve_solutions_specific_sequence(user_id, dataset, model, strategy, 1048576, top_k, True, sim_matrix, best_interactions, target_item, worst_jaccard_sample, alpha=0.5, normalization='default')\n",
    "    \n",
    "    print(\"neg_original_interactions\", worst_jacc_search_info[0].interactions['best'])\n",
    "\n",
    "    last_predictions_reverse = -model.predict(worst_jacc_search_info[0].interactions['best'])\n",
    "    last_predictions_reverse[worst_jacc_search_info[0].interactions['best']] = StaticVars.FLOAT_MAX\n",
    "    \n",
    "    last_pos_target_item_reverse = np.where(last_predictions_reverse.argsort() == target_item)[0][0] #bug maybe?\n",
    "    print(\"last pos\",  last_pos_target_item_reverse)\n",
    "    print(\"item : \", last_predictions_reverse.argsort()[last_pos_target_item_reverse])\n",
    "\n",
    "    # worst_jacc_cfs = np.setdiff1d( worst_jacc_search_info[0].interactions['best'], best_interactions)\n",
    "    \n",
    "    if are_the_same(worst_jacc_search_info[0].interactions['best'], original_interactions):\n",
    "        # counter = counter + 1\n",
    "        print(\"Hopefully they are the same!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_items_if_missing(items_removed, target_list):\n",
    "    items_to_replace = set(items_removed) - set(target_list)\n",
    "    for i, item in enumerate(items_to_replace):\n",
    "        j = i\n",
    "        while target_list[j] in items_removed:\n",
    "            j += 1\n",
    "        target_list[j] = item\n",
    "    # should maybe shuffle?\n",
    "\n",
    "def create_reverse_mode_evaluation_dataframe(dataset, model, strategy, target_item_pos, top_k, sim_matrix):\n",
    "    evaluation_df = pd.DataFrame(columns=['user_id', 'target_pos', 'original_interactions', 'cfs', 'len_cfs'])#, 'worst_jacc_sample', 'worst_jacc_cfs', 'len_worst_jacc_cfs'])#, 'jacc_cfs', 'len_jacc_cfs', 'rs_cfs', 'len_rs_cfs'])\n",
    "    len_sample = 20\n",
    "\n",
    "    for user_id in tqdm.notebook.tqdm(range(1, max(dataset.user_ids))):\n",
    "        for target_pos in target_item_pos:\n",
    "            specific_cfs = _find_specific_cfs_(dataset, model, strategy, target_pos, False, sim_matrix, user_id, 1000, top_k, alpha=0.5, normalization='default')\n",
    "            \n",
    "            user_sequences = test.sequences[test.user_ids == user_id]\n",
    "            \n",
    "            for j in range (min(1, len(user_sequences))):\n",
    "                if all(v > 0 for v in user_sequences[j]):\n",
    "                    original_interactions = user_sequences[j].copy()\n",
    "                    best_interactions = specific_cfs[j].interactions['best']\n",
    "                    items_removed = np.setdiff1d(original_interactions, best_interactions)\n",
    "                    new_lign = {'user_id': user_id, 'target_pos': target_pos, 'cfs': items_removed, 'len_cfs': len(items_removed)}#, 'worst_jacc_cfs': None, 'len_worst_jacc_cfs': None}#, jacc_search_info[0].interactions['best'], len(jacc_search_info[0].interactions['best']), rs_search_info[0].interactions['best'], len(rs_search_info[0].interactions['best'])])\n",
    "                    evaluation_df = evaluation_df.append(new_lign, ignore_index=True)\n",
    "                    predictions = -pretrained_models['lstm'].predict(original_interactions)\n",
    "                    predictions[original_interactions] = StaticVars.FLOAT_MAX\n",
    "                    target_item = predictions.argsort()[min(top_k, target_pos)]\n",
    "                    if len(best_interactions) == 0 : \n",
    "                        print(\"empty\", user_id)\n",
    "                        new_lign = {'user_id': user_id, 'target_pos': target_pos, 'cfs': items_removed, 'len_cfs': len(items_removed), 'worst_jacc_cfs': None, 'len_worst_jacc_cfs': None}#, jacc_search_info[0].interactions['best'], len(jacc_search_info[0].interactions['best']), rs_search_info[0].interactions['best'], len(rs_search_info[0].interactions['best'])])\n",
    "                        evaluation_df = evaluation_df.append(new_lign, ignore_index=True)\n",
    "                        break\n",
    "                    predictions_reverse = -pretrained_models['lstm'].predict(best_interactions)\n",
    "                    predictions_reverse[best_interactions] = StaticVars.FLOAT_MAX\n",
    "                    pos_target_item_reverse = np.where(predictions_reverse.argsort() == target_item)[0][0] #bug maybe?\n",
    "                    worst_jaccard_sample = find_sample_with_jaccard(target_item, best_interactions, jaccard_sims_matrix, len_sample, worst_items = True)\n",
    "                    jaccard_sample = find_sample_with_jaccard(target_item, best_interactions, jaccard_sims_matrix, len_sample, worst_items = True)\n",
    "                    rs_sample = find_best_items_using_recommender(target_item, best_interactions, pretrained_models['lstm'], len_sample)\n",
    "\n",
    "                    if len(items_removed) >= len_sample:\n",
    "                        print(f\"Sequence skipped, too much items removed for user_id {user_id}.\")\n",
    "                        continue\n",
    "                    \n",
    "                    if not set(items_removed) <= set(jaccard_sample):\n",
    "                        replace_items_if_missing(items_removed, jaccard_sample)\n",
    "                        jacc_search_info = retrieve_solutions_specific_sequence(user_id, test, pretrained_models['lstm'], get_backend_strategy('combo'), 1000, top_k, True, jaccard_sims_matrix, best_interactions, target_item, jaccard_sample, alpha=0.5)\n",
    "\n",
    "                    if not set(items_removed) <= set(rs_sample):\n",
    "                        replace_items_if_missing(items_removed, rs_sample)\n",
    "                        rs_search_info = retrieve_solutions_specific_sequence(user_id, test, pretrained_models['lstm'], get_backend_strategy('combo'), 1000, top_k, True, jaccard_sims_matrix, best_interactions, target_item, rs_sample, alpha=0.5)\n",
    "\n",
    "                    if not set(items_removed) <= set(worst_jaccard_sample):\n",
    "                        replace_items_if_missing(items_removed, worst_jaccard_sample)\n",
    "                        worst_jacc_search_info = retrieve_solutions_specific_sequence(user_id, test, pretrained_models['lstm'], get_backend_strategy('combo'), 1000, top_k, True, jaccard_sims_matrix, best_interactions, pos_target_item_reverse, worst_jaccard_sample, alpha=0.5, normalization='default')\n",
    "                    worst_jacc_cfs = np.setdiff1d( worst_jacc_search_info[j].interactions['best'], best_interactions)\n",
    "                    new_lign = {'user_id': user_id, 'original_interactions': original_interactions, 'target_pos': target_pos, 'cfs': items_removed, 'len_cfs': len(items_removed), 'worst_jacc_sample': worst_jaccard_sample, 'worst_jacc_cfs': worst_jacc_search_info[j].interactions['best'], 'len_worst_jacc_cfs': len(worst_jacc_search_info[j].interactions['best'])}#, jacc_search_info[0].interactions['best'], len(jacc_search_info[0].interactions['best']), rs_search_info[0].interactions['best'], len(rs_search_info[0].interactions['best'])])\n",
    "                    evaluation_df = evaluation_df.append(new_lign, ignore_index=True)\n",
    "    return evaluation_df\n",
    "# df3 = create_reverse_mode_evaluation_dataframe(test, pretrained_models['lstm'], get_backend_strategy('combo'), [1, 3, 5, 7], 10, jaccard_sims_matrix)\n",
    "# %store df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple class stack that only allows pop and push operations\n",
    "class Stack:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stack = []\n",
    "\n",
    "    def pop(self):\n",
    "        if len(self.stack) < 1:\n",
    "            return None\n",
    "        return self.stack.pop()\n",
    "\n",
    "    def push(self, item):\n",
    "        self.stack.append(item)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.stack)\n",
    "\n",
    "\n",
    "# And a queue that only has enqueue and dequeue operations\n",
    "class Queue:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.queue = []\n",
    "\n",
    "    def enqueue(self, item):\n",
    "        self.queue.append(item)\n",
    "\n",
    "    def dequeue(self):\n",
    "        if len(self.queue) < 1:\n",
    "            return None\n",
    "        return self.queue.pop(0)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.queue)\n",
    "\n",
    "    def clear(self):\n",
    "        del self.queue[:]\n",
    "\n",
    "    def get(self, i):\n",
    "        return self.queue[i]\n",
    "    \n",
    "    def setter(self, i, v):\n",
    "        self.queue[i] = v"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
